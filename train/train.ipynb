{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dcafarelli/CMT-ABAW2020-EXPR/blob/main/train/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B--nvvyZnVRq"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import cv2\n",
        "import shutil\n",
        "import sys\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "import sklearn.metrics as sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8UOWdDC3AKi"
      },
      "source": [
        "# GENERAL SETTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wearBO1j3JDK"
      },
      "source": [
        "#CUDA FOR PYTORCH\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True #This flag allows you to enable the inbuilt cudnn auto-tuner to find the best algorithm to use for your hardware.\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox7JNzKp3F0G"
      },
      "source": [
        "#--------- Dataset settings ---------\n",
        "#--------- Dataset settings ---------\n",
        "downsampling_prob = 0.1\n",
        "resize_algo = Image.BILINEAR\n",
        "valid_resolution = 112\n",
        "\n",
        "#Weights = max(num of occorence)/num of occorence\n",
        "loss_weights = [1, 22.92, 37.50, 50.66, 3.47, 5.79, 13.55]\n",
        "loss_weights_nt = [1, 3.47, 3.81]\n",
        "loss_weights = torch.FloatTensor(loss_weights).to(device)\n",
        "loss_weights_nt = torch.FloatTensor(loss_weights_nt).to(device)\n",
        "\n",
        "batch_size = 64\n",
        "batch_val_size = 32 #different from the above to avoid CUDA Out of Memory\n",
        "classes = ('Neutral', 'Anger', 'Disgust', 'Fear', 'Happiness', 'Sadness', 'Surprise')\n",
        "classes_nt = ('Neutral', 'Positive', 'Negative')\n",
        "\n",
        "#--------- Training settings ---------\n",
        "iteration_step = 3920\n",
        "curr_step = 35000\n",
        "batch_accumulation = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEmsOhFp7E-R"
      },
      "source": [
        "# --------- PATHS ---------\n",
        "\n",
        "#--------- path to dataframe (path/label) --> AffWild2 + Expw Dataset ---------\n",
        "train_df_path = '/annotations/aff_wild_expW_train_set.pkl'\n",
        "val_df_path = '/annotations/val_set.pkl'\n",
        "\n",
        "#--------- path to dataframe (path/label) --> \"new task\" Dataset ---------\n",
        "train_df_nt_path = '/annotations/three_classes_label.pkl'\n",
        "val_df_nt_path = '/annotations/three_classes_val_label.pkl'\n",
        "\n",
        "#--------- ckp path to load the model ---------\n",
        "model_base_path_colab = '/model_checkpoint/pytorch_models/senet50_ft_pytorch.pth'\n",
        "model_ckp_path = '/model_checkpoint/pytorch_models/models_ckp_78561.pth.tar'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnPEnSVOZJKL"
      },
      "source": [
        "def subtract_mean(x):\n",
        "    mean_vector = [91.4953, 103.8827, 131.0912]\n",
        "    x *= 255.\n",
        "    x[0] -= mean_vector[0]\n",
        "    x[1] -= mean_vector[1]\n",
        "    x[2] -= mean_vector[2]\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaHE5lkibcWp"
      },
      "source": [
        "#DATA AUGMENTATION\n",
        "\n",
        "transformed_train = transforms.Compose([                                     \n",
        "                      transforms.Resize((256,256)),\n",
        "                      transforms.RandomCrop((224,224)),\n",
        "                      transforms.ColorJitter(brightness=0.4, contrast = 0.3, saturation = 0.25, hue = 0.05),\n",
        "                      transforms.RandomHorizontalFlip(p=0.5),\n",
        "                      transforms.ToTensor(),\n",
        "                      transforms.Lambda(lambda x : subtract_mean(x))\n",
        "])\n",
        "\n",
        "transformed_val = transforms.Compose([\n",
        "                      transforms.Resize((224,224)),\n",
        "                      transforms.ToTensor(),\n",
        "                      transforms.Lambda(lambda x : subtract_mean(x))\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esX04DiBS7H2"
      },
      "source": [
        "# \"STANDARD\" DATASET\n",
        "\n",
        "Run the cells below if you want to train your model with samples at the same *resolution*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxDB5MqJXHEK"
      },
      "source": [
        "class AffWild2Dataset(Dataset):\n",
        "    def __init__(self, choose_set, flag, transform=None):\n",
        "        self.flag = flag\n",
        "        self.choose_set = choose_set\n",
        "        \n",
        "        if choose_set == 'affwild2': \n",
        "          if flag == 'train':\n",
        "              pkl_path = train_df_path \n",
        "          else:\n",
        "              pkl_path = val_df_path\n",
        "        else:\n",
        "          if flag == 'train':\n",
        "              pkl_path = train_df_nt_path \n",
        "          else:\n",
        "              pkl_path = val_df_nt_path\n",
        "   \n",
        "        self.emotion_frame = pd.read_pickle(pkl_path)\n",
        "        self.transform = transform\n",
        "   \n",
        "    def __len__(self):\n",
        "        return len(self.emotion_frame)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        if self.flag == 'train':\n",
        "            img_path = self.emotion_frame.iloc[index, 0]           \n",
        "            fp = os.path.join('/cropped_aligned_train%s' %img_path) #here the path to training frames\n",
        "        else:\n",
        "            img_path = self.emotion_frame.iloc[index, 0]\n",
        "            fp = os.path.join('/cropped_aligned_val%s' %img_path) #here the path to validation frames\n",
        "        \n",
        "        img_array = Image.fromarray(cv2.imread(fp))\n",
        "        \n",
        "        y_label = self.emotion_frame['label'].values[index]\n",
        "        \n",
        "        if self.transform:\n",
        "            img_array = self.transform(img_array)\n",
        "        \n",
        "        return img_array, y_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rqm66AFNsf8a"
      },
      "source": [
        "#DATASET CREATION\n",
        "\n",
        "train_set = AffWild2Dataset(flag = 'train', choose_set = 'affwild2', transform=transformed_train)\n",
        "\n",
        "validation_set = AffWild2Dataset(flag = 'validation',choose_set = 'affwild2', transform=transformed_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihc1JEzarmk9"
      },
      "source": [
        "# \"MULTI-RESOLUTION\" DATASET\n",
        "Run the cells below if you want to train your model with samples at different resolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH13whbgr5Dp"
      },
      "source": [
        "class AffWild2Dataset(Dataset):\n",
        "    def __init__(self, flag, choose_set, resize_algo, downsampling_prob, valid_resolution, curr_step, transform=None):\n",
        "        self.flag = flag\n",
        "        self.choose_set = choose_set\n",
        "        \n",
        "        if choose_set == 'affwild2':\n",
        "          if flag == 'train':\n",
        "              pkl_path = train_df_path  \n",
        "              self.downsampling_prob = downsampling_prob\n",
        "          else:\n",
        "              pkl_path = val_df_path\n",
        "              self.downsampling_prob = 1.0\n",
        "        else:\n",
        "          if flag == 'train':\n",
        "              pkl_path = train_df_nt_path  \n",
        "              self.downsampling_prob = downsampling_prob\n",
        "          else:\n",
        "              pkl_path = val_df_nt_path\n",
        "              self.downsampling_prob = 1.0\n",
        "\n",
        "        self.emotion_frame = pd.read_pickle(pkl_path)\n",
        "        self.transform = transform\n",
        "        self.resize_algo = resize_algo\n",
        "        self.valid_resolution = valid_resolution\n",
        "        self._loader = self._get_loader\n",
        "        self.curr_index = 0\n",
        "        self.curr_step = curr_step\n",
        "    \n",
        "    @staticmethod\n",
        "    def _get_loader(path):\n",
        "        return Image.fromarray(cv2.imread(path))\n",
        "\n",
        "    def _lower_resolution(self, img):\n",
        "        w_i, h_i = img.size\n",
        "        r = h_i/float(w_i)\n",
        "        if self.flag == 'train':\n",
        "            res = torch.rand(1).item()\n",
        "            res = 3 + 5*res\n",
        "            res = 2**int(res)\n",
        "        else:\n",
        "            res = self.valid_resolution\n",
        "        if res >= w_i or res >= h_i:\n",
        "            return img\n",
        "        if h_i < w_i:\n",
        "            h_n = res\n",
        "            w_n = h_n/float(r)\n",
        "        else:\n",
        "            w_n = res\n",
        "            h_n = w_n*float(r)\n",
        "        img2 = img.resize((int(w_n), int(h_n)), self.resize_algo)\n",
        "        img2 = img2.resize((w_i, h_i), self.resize_algo)\n",
        "        return img2\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.emotion_frame)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        if self.flag == 'train':\n",
        "            self.curr_index +=1\n",
        "            img_path = self.emotion_frame.iloc[index, 0]           \n",
        "            fp = os.path.join('/content/cropped_aligned_train%s' %img_path) #here the path to training frames\n",
        "\n",
        "            if (self.curr_index % self.curr_step) == 0 and self.downsampling_prob < 1.0 :\n",
        "              self.downsampling_prob += 0.1\n",
        "        else:\n",
        "            img_path = self.emotion_frame.iloc[index, 0]\n",
        "            fp = os.path.join('/content/cropped_aligned_val%s' %img_path) #here the path to validation frames\n",
        "        \n",
        "        img = self._loader(fp)\n",
        "\n",
        "        if torch.rand(1).item() < self.downsampling_prob:\n",
        "            img = self._lower_resolution(img)\n",
        "        \n",
        "        y_label = self.emotion_frame['label'].values[index]\n",
        "        \n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        \n",
        "        return img, y_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkHBLk7nso53"
      },
      "source": [
        "#DATASET CREATION\n",
        "\n",
        "train_set = AffWild2Dataset(flag = 'train', choose_set = 'balanced', resize_algo = resize_algo, downsampling_prob = downsampling_prob, \n",
        "                            valid_resolution = valid_resolution, curr_step = curr_step, transform=transformed_train)\n",
        "\n",
        "validation_set = AffWild2Dataset(flag = 'validation', choose_set = 'balanced', resize_algo = resize_algo, downsampling_prob = downsampling_prob, \n",
        "                            valid_resolution = valid_resolution, curr_step = curr_step, transform=transformed_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfWKj_qLtmLI"
      },
      "source": [
        "# DATASET GENERATOR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWMpirJM0m_B"
      },
      "source": [
        "\n",
        "#DATA GENERATORS WITH SAMPLER TO BALANCE THE DATASET\n",
        "\n",
        "train_generator = DataLoader(train_set, batch_size = batch_size, shuffle=True, num_workers=8 ,pin_memory=True, drop_last=True)\n",
        "\n",
        "validation_generator = DataLoader(validation_set, batch_size = batch_val_size, num_workers = 8,  pin_memory=True, drop_last=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSgRNdK6Iy01"
      },
      "source": [
        "\n",
        "# MODEL CONFIGURATION\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffAE7gY_3JDM"
      },
      "source": [
        "sys.path.append('/path/where/MainModel.py/is_located') #append the path where MainModel.py is located\n",
        "import MainModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WruaPF8v1Lbc"
      },
      "source": [
        "def load_models(model_base_path, device=\"cpu\", model_ckp=None):\n",
        "    assert os.path.exists(model_base_path), \"Base model checkpoint not found at: {}\".format(model_base_path)\n",
        "    model = torch.load(model_base_path)\n",
        "    if model_ckp is not None:\n",
        "        assert os.path.exists(model_ckp), f\"Model checkpoint not found at: {model_ckp}\"\n",
        "        ckp = torch.load(model_ckp, map_location='cpu')\n",
        "        [p.data.copy_(torch.from_numpy(ckp['model_state_dict'][n].numpy())) for n, p in model.named_parameters()]\n",
        "        for n, m in model.named_modules():\n",
        "            if isinstance(m, nn.BatchNorm2d):\n",
        "                m.momentum = 0.1\n",
        "                m.running_var = ckp['model_state_dict'][n + '.running_var']\n",
        "                m.running_mean = ckp['model_state_dict'][n + '.running_mean']\n",
        "                m.num_batches_tracked = ckp['model_state_dict'][n + '.num_batches_tracked']\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGNooi4n152w"
      },
      "source": [
        "model = load_models(model_base_path_colab, device, model_ckp_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI4hXg5FsxUV"
      },
      "source": [
        "for k, m in model.named_modules():\n",
        "  m._non_persistent_buffers_set = set()  # pytorch 1.6.0 compatability"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDGBos_CawQE"
      },
      "source": [
        "def reshape(flag, model):\n",
        "  if flag == \"affwild2\":\n",
        "    model.classifier_1 = nn.Linear(2048, len(classes))\n",
        "  else \n",
        "    model.classifier_1 = nn.Linear(2048, len(classes_nt))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33UJT0eSJkEk"
      },
      "source": [
        "model = reshape(\"affwild2\", model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mqjoi1eLtB7e"
      },
      "source": [
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PldIR_fSdaf7"
      },
      "source": [
        "my_list = ['classifier_1.weight', 'classifier_1.bias']\n",
        "params = list(map(lambda x: x[1],list(filter(lambda kv: kv[0] in my_list, model.named_parameters()))))\n",
        "base_params = list(map(lambda x: x[1],list(filter(lambda kv: kv[0] not in my_list, model.named_parameters()))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhUwhYiX8bN7"
      },
      "source": [
        "def criterion(flag):\n",
        "  if flag == \"affwild2\":\n",
        "    criterion = nn.CrossEntropyLoss(weight = loss_weights)\n",
        "  else:\n",
        "    criterion = nn.CrossEntropyLoss(loss_weights_nt)\n",
        "  return criterion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-Xz6OWMcEN6"
      },
      "source": [
        "#LOSS FUNCTION AND OPTIMIZER\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight = new_weights)\n",
        "\n",
        "#Different learning rate for fine tuning the network\n",
        "optimizer = optim.SGD([{'params': base_params}, \n",
        "                       {'params': params, 'lr': 1e-4}], lr=1e-6, momentum=0.9, weight_decay = 5e-3)\n",
        "\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, 10, 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAEB2vt0byP0"
      },
      "source": [
        "# TRAINING SECTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-JzSX7_tIb9"
      },
      "source": [
        "#Saving function\n",
        "\n",
        "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
        "    \"\"\"\n",
        "    state: checkpoint \n",
        "    is_best: is this the best checkpoint; \n",
        "    checkpoint_path: path to save checkpoint\n",
        "    best_model_path: path to save best model\n",
        "    \"\"\"\n",
        "    f_path = checkpoint_path\n",
        "    # save checkpoint data to the path given, checkpoint_path\n",
        "    torch.save(state, f_path)\n",
        "    if is_best:\n",
        "        best_fpath = best_model_path\n",
        "        # copy that checkpoint file to best path given, best_model_path\n",
        "        shutil.copyfile(f_path, best_fpath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwupNkTdMFnX"
      },
      "source": [
        "#Loading function\n",
        "\n",
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "\n",
        "    # load check point\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    # initialize state_dict from checkpoint to model\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    # initialize optimizer from checkpoint to optimizer\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
        "    best_stat = checkpoint['best_stat']\n",
        "    train_loss = checkpoint['train_loss']\n",
        "    val_loss = checkpoint['val_loss']\n",
        "    train_acc = checkpoint['train_acc']\n",
        "    val_acc = checkpoint['val_acc']\n",
        "    return model, optimizer, checkpoint['epoch'], best_stat, train_loss, val_loss, train_acc, val_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MljUD6FKp-TQ"
      },
      "source": [
        "def metrics(lab, pred):\n",
        "  lab_array = [t.numpy() for t in lab]\n",
        "  pred_array = [t.numpy() for t in pred]\n",
        "\n",
        "  pred_array = np.concatenate(pred_array, axis=0 )\n",
        "  lab_array = np.concatenate(lab_array, axis=0)\n",
        "\n",
        "  F1_score = sm.f1_score(lab_array, pred_array, average='macro', zero_division=1)\n",
        "  classes_score = sm.f1_score(lab_array, pred_array, average=None, zero_division=1)\n",
        "  print(\"Acc classes \", classes_score)\n",
        "  accuracy = sm.accuracy_score(lab_array, pred_array)\n",
        "  confusion_matrix = sm.confusion_matrix(lab_array, pred_array)\n",
        "  \n",
        "  return accuracy, F1_score, confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE-YNP4a0vXJ"
      },
      "source": [
        "#STATISTIC COMPETITION\n",
        "def stat_comp(F1_score, accuracy):\n",
        "  stat = (0.33*accuracy) + (0.67*F1_score)\n",
        "  return stat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlcMvDmLziNx"
      },
      "source": [
        "def evaluate(model):\n",
        "\n",
        "  running_val_loss = 0.0\n",
        "  total = 0\n",
        "\n",
        "  pred = []\n",
        "  lab = []\n",
        "\n",
        "  model.eval()\n",
        "  print(\"Enter Evaluation. Is Training?\", model.training)\n",
        "  with torch.no_grad():\n",
        "    for j, (data) in enumerate(progress_bar(validation_generator)):\n",
        "\n",
        "      faces_val, labels_val = data\n",
        "      faces_val = faces_val.to(device)\n",
        "      labels_val = labels_val.to(device)\n",
        "\n",
        "      _, outputs_val = model(faces_val)\n",
        "\n",
        "      loss_val = criterion(outputs_val, labels_val)\n",
        "      _, preds_val = torch.max(outputs_val.data, 1)\n",
        "\n",
        "      running_val_loss += loss_val.item()*faces_val.size(0)\n",
        "      \n",
        "      pred.append(preds_val.cpu())\n",
        "      lab.append(labels_val.cpu())\n",
        "      \n",
        "      total += labels_val.size(0)\n",
        "          \n",
        "  iteration_val_loss = running_val_loss / total\n",
        "  iteration_val_acc, F1_score, cm = metrics(lab, pred)\n",
        "              \n",
        "  return iteration_val_loss, iteration_val_acc, F1_score, cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by2rT1GKkRhT"
      },
      "source": [
        "def train(start_epochs, n_epochs, best_stat, classes, train_generator, val_generator, train_loss, val_loss, train_acc, val_acc, model, optimizer, criterion, checkpoint_path, best_model_path):\n",
        " \n",
        "  class_correct = list(0. for i in range(len(classes)))\n",
        "  class_total = list(0. for i in range(len(classes)))\n",
        "  accumulation_step = \n",
        "\n",
        "  for epoch in range(start_epochs, n_epochs+1):\n",
        "\n",
        "    running_train_loss = 0.0\n",
        "    running_train_corrects = 0.0\n",
        "\n",
        "    total_t = 0\n",
        "    #Training\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    for k, (data) in enumerate(progress_bar(train_generator)):\n",
        "\n",
        "        faces, labels = data\n",
        "        faces = faces.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        #forward\n",
        "        _,output = model(faces)\n",
        "\n",
        "        # Compute loss  \n",
        "        loss = criterion(output, labels)\n",
        "         #predictions of the model determined using the torch.max() function, which returns the index of the maximum value in a tensor.\n",
        "        _, preds = torch.max(output.data, 1)\n",
        "\n",
        "        #optimizer.zero_grad()\n",
        "        running_train_loss += loss.item()*faces.size(0) \n",
        "        running_train_corrects += (preds == labels).sum().item()\n",
        "\n",
        "        total_t += labels.size(0)\n",
        "\n",
        "        c = (preds == labels).squeeze()\n",
        "        for i in range(faces.size()[0]):\n",
        "          label = labels[i]\n",
        "          class_correct[label] += c[i].item()\n",
        "          class_total[label] += 1\n",
        "\n",
        "        # Backpropagate the gradients\n",
        "        loss.backward()\n",
        "        if(k+1) % batch_accumulation == 0:\n",
        "          # Update the parameters\n",
        "          optimizer.step()\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          if (k + 1) % iteration_step == 0:\n",
        "\n",
        "            # calculate average losses and accuracy train\n",
        "            iteration_train_loss = running_train_loss / total_t\n",
        "            iteration_train_acc = (running_train_corrects / total_t) * 100\n",
        "            train_loss.append(iteration_train_loss)\n",
        "            train_acc.append(iteration_train_acc)\n",
        "            print(\"'Total after iter\", total_t)\n",
        "\n",
        "            print('---------------------Iteration: %d ---------------------' %k)\n",
        "\n",
        "            print('Train Loss: {:.4f} Train Acc: {:.2f}%'.format(iteration_train_loss, iteration_train_acc))\n",
        "            # calculate prediction of each class\n",
        "            for i in range(7):\n",
        "              print('Train Accuracy of %5s : %2d %%' % (classes[i], 100 * class_correct[i] / class_total[i]))\n",
        "\n",
        "            iteration_val_loss, iteration_val_acc, F1_score, cm = evaluate(model)\n",
        "            final_stat = stat_comp(F1_score, iteration_val_acc)\n",
        "\n",
        "            val_loss.append(iteration_val_loss)\n",
        "            val_acc.append(final_stat)\n",
        "\n",
        "            print('Validation Loss: {:.4f} Validation Acc: {:.2f}'.format(iteration_val_loss, iteration_val_acc))\n",
        "            print('F1_Score : {:.4f}'.format(F1_score))\n",
        "\n",
        "            print('Final statistics: {:.4f}'.format(final_stat))\n",
        "            print('_________________________________________________________')\n",
        "\n",
        "            scheduler.step()\n",
        "            print(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "            running_train_loss = 0.0\n",
        "            running_train_corrects = 0.0\n",
        "            total_t = 0\n",
        "            print(\"Total before iter\", total_t)\n",
        "            model.train()\n",
        "            print(\"Evaluation ended. Is training?\", model.training)\n",
        "\n",
        "            # create checkpoint variable and add important data\n",
        "            checkpoint = {\n",
        "                'iteration' :i + 1,\n",
        "                'epoch': epoch + 1,\n",
        "                'valid_loss_min': iteration_val_loss,\n",
        "                'best_stat': best_stat,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'train_loss': train_loss,\n",
        "                'val_loss': val_loss,\n",
        "                'train_acc': train_acc,\n",
        "                'val_acc':val_acc\n",
        "                }\n",
        "            \n",
        "            # save checkpoint\n",
        "            save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n",
        "            \n",
        "            if(final_stat > best_stat):\n",
        "              print('Statistic increases ({:.6f} --> {:.6f}).  Saving model ...'.format(best_stat,final_stat))\n",
        "              best_stat = final_stat\n",
        "              save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
        "      \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw6XFHZntY4-"
      },
      "source": [
        "checkpoint_path = '/checkpoint/ckp_model.pt'\n",
        "best_model_path = '/best_model/best_model.pt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpJr-ufL1hel"
      },
      "source": [
        "train_loss = []\n",
        "val_loss = []\n",
        "train_acc = []\n",
        "val_acc = []\n",
        "avg_train_acc = []\n",
        "avg_val_acc = []\n",
        "\n",
        "#TRAINING AND VALIDATION \n",
        "trained_model = train(1, 20, 0.0, classes, train_generator, validation_generator, train_loss, val_loss, train_acc, val_acc, model, optimizer, criterion, checkpoint_path, best_model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiJ6Tw6pjtzW"
      },
      "source": [
        "#LOAD SAVED CHECKPOINT\n",
        "\n",
        "model_t, optimizer, start_epoch, best_stat, train_loss, val_loss, train_acc, val_acc = load_ckp(checkpoint_path,model, optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}