{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_competition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dcafarelli/CMT-ABAW2020-EXPR/blob/main/test/test_competition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QwKIEdxuDc9"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import sys\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "import sklearn.metrics as sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySo7Tm-GLQeW"
      },
      "source": [
        "#CUDA FOR PYTORCH\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True #This flag allows you to enable the inbuilt cudnn auto-tuner to find the best algorithm to use for your hardware.\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIzqN_FomoHl"
      },
      "source": [
        "path_best_model = '/best_performance_model/best_model-senet50-0.43.pt'\n",
        "\n",
        "#--------- ckp path to load the model ---------\n",
        "model_base_path_colab = '/content/gdrive/My Drive/TESI/FER/AffWild2/model_checkpoint/pytorch_models/senet50_ft_pytorch.pth'\n",
        "model_ckp_path = '/content/gdrive/MyDrive/TESI/FER/AffWild2/model_checkpoint/pytorch_models/models_ckp_78561.pth.tar'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UMHvVMKd35z"
      },
      "source": [
        "test_set_frames_path = '/Test_Set/test_set_only_path.pkl' #dataframe with test frames path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viUGlAlALVGs"
      },
      "source": [
        "# TEST SET CREATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAD2UVuQLZHt"
      },
      "source": [
        "class AffWild2TestSet(Dataset):\n",
        "    def __init__(self, transform=None):\n",
        "\n",
        "      pkl_path = test_set_frames_path\n",
        "\n",
        "      self.emotion_frame = pd.read_pickle(pkl_path)\n",
        "      self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.emotion_frame)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "\n",
        "      img_path = self.emotion_frame.iloc[index, 0]           \n",
        "      fp = os.path.join('/content/cropped_aligned_test%s' %img_path) #here the path to test frames\n",
        "      assert os.path.exists(fp), \"Image not found at: {}\".format(fp)\n",
        "        \n",
        "      test_set_face = Image.fromarray(cv2.imread(fp))\n",
        "      if self.transform:\n",
        "        test_set_face = self.transform(test_set_face)\n",
        "        \n",
        "      return test_set_face"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqIU4mJFhF-_"
      },
      "source": [
        "#DATA TRANSFORMATION\n",
        "def subtract_mean(x):\n",
        "    mean_vector = [91.4953, 103.8827, 131.0912]\n",
        "    x *= 255.\n",
        "    x[0] -= mean_vector[0]\n",
        "    x[1] -= mean_vector[1]\n",
        "    x[2] -= mean_vector[2]\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMyAOpHiMRWg"
      },
      "source": [
        "transformed_test = transforms.Compose([\n",
        "                      transforms.Resize((224,224)),\n",
        "                      transforms.ToTensor(),\n",
        "                      transforms.Lambda(lambda x : subtract_mean(x))\n",
        "                      ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK9zkh9XMX1r"
      },
      "source": [
        "test_set = AffWild2TestSet(transform=transformed_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgO7l2N-NNA-"
      },
      "source": [
        "test_generator = DataLoader(test_set, batch_size = 64, num_workers = 0, pin_memory=True)\n",
        "\n",
        "classes = ('Neutral', 'Anger', 'Disgust', 'Fear', 'Happiness', 'Sadness', 'Surprise')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEBXjSiDlnqF"
      },
      "source": [
        "# LOAD BEST MODEL SENET50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mmaK86Kl9eI"
      },
      "source": [
        "sys.path.append('/path/where/MainModel.py/is_located') #append the path where MainModel.py is located\n",
        "import MainModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peVQjClrXNZb"
      },
      "source": [
        "def load_models(model_base_path, device=\"cpu\", model_ckp=None):\n",
        "    assert os.path.exists(model_base_path), \"Base model checkpoint not found at: {}\".format(model_base_path)\n",
        "    model = torch.load(model_base_path)\n",
        "    if model_ckp is not None:\n",
        "        assert os.path.exists(model_ckp), f\"Model checkpoint not found at: {model_ckp}\"\n",
        "        ckp = torch.load(model_ckp, map_location='cpu')\n",
        "        [p.data.copy_(torch.from_numpy(ckp['model_state_dict'][n].numpy())) for n, p in model.named_parameters()]\n",
        "        for n, m in model.named_modules():\n",
        "            if isinstance(m, nn.BatchNorm2d):\n",
        "                m.momentum = 0.1\n",
        "                m.running_var = ckp['model_state_dict'][n + '.running_var']\n",
        "                m.running_mean = ckp['model_state_dict'][n + '.running_mean']\n",
        "                m.num_batches_tracked = ckp['model_state_dict'][n + '.num_batches_tracked']\n",
        "    \n",
        "    return model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da52B6GymDvE"
      },
      "source": [
        "model = load_models(model_base_path_colab, device, None)\n",
        "\n",
        "for k, m in model.named_modules():\n",
        "  m._non_persistent_buffers_set = set()  # pytorch 1.6.0 compatability"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbuyddEMmM4e"
      },
      "source": [
        "model.classifier_1 = nn.Linear(2048, len(classes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhstbA5j0NFX"
      },
      "source": [
        "#FREEZING ALL THE LAYER EXCEPT THE FULLY CONNECTED ONE\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYrNcQmkmQyz"
      },
      "source": [
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzZY01ZVmolj"
      },
      "source": [
        "#Load state dict\n",
        "ckp = torch.load(path_best_model)\n",
        "model.load_state_dict(ckp['state_dict'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4gyqzgWRLGV"
      },
      "source": [
        "# PREDICTION ON TEST SET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKaNLECtsdM_"
      },
      "source": [
        "def inference(model):\n",
        "  pred = []\n",
        "\n",
        "  model.eval()\n",
        "  print(\"Enter Evaluation. Is Training?\", model.training)\n",
        "  with torch.no_grad():\n",
        "    for j, faces in enumerate(progress_bar(test_generator)):\n",
        "\n",
        "      faces = faces.to(device)\n",
        "\n",
        "      _, outputs = model(faces)\n",
        "      _, predictions = torch.max(outputs.data, 1)\n",
        "\n",
        "      pred.append(predictions.cpu())\n",
        "           \n",
        "  return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_YOtQkat2tT"
      },
      "source": [
        "predictions = inference(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-w9taO2tzKg"
      },
      "source": [
        "pred_array = [t.numpy() for t in predictions]\n",
        "\n",
        "pred_array = np.concatenate(pred_array, axis=0 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Unq-wLBN9BWs"
      },
      "source": [
        "#store predictions array \n",
        "np.save('/Test_Set/test_inference.npy', pred_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76MdNzCXMfkM"
      },
      "source": [
        "# WRITE LABEL ON FILES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h6FpmtRM3QR"
      },
      "source": [
        "lab_array = np.load('/Test_Set/test_inference.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TxoA8Q3UcQP"
      },
      "source": [
        "def ex_to_str(arr):\n",
        "  str = \"{:d}\".format(arr)\n",
        "  return str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qflBeT5OEipN"
      },
      "source": [
        "#write array values on a txt file\n",
        "\n",
        "text_file = open('/Test_Set/test_inference.txt', \"w\")\n",
        "# write the values\n",
        "for i in range(lab_array.shape[0]):\n",
        "  n = text_file.write(ex_to_str(lab_array[i]))\n",
        "  n = text_file.write('\\n')\n",
        "text_file.close()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH4e53JAlQAk"
      },
      "source": [
        "def create_test_output(filename):\n",
        "  d = dict()\n",
        "  with open(filename,'r') as fp:\n",
        "    lines = fp.readlines()#.sort()\n",
        "    lines.sort()\n",
        "    lines = [line.strip().split('/')[1:3] for line in lines]\n",
        "    print('\\ntotal: ', len(lines), lines[:2])#,lines[-20:])\n",
        "    \n",
        "    for line in lines:\n",
        "      key,value = line[0], line[1]\n",
        "      #print(value)\n",
        "      \n",
        "      video_file = '/Test_Set/predictions/'+key+'.txt'\n",
        "      if not os.path.exists(video_file):\n",
        "        f = open(video_file,'w')\n",
        "        f.write('Neutral,Anger,Disgust,Fear,Happiness,Sadness,Surprise')\n",
        "      else:\n",
        "        f = open(video_file,'a') \n",
        "            \n",
        "      name, emotion = value.replace(\"'\",\"\").split(';')#v.replace(\"'\",\"\").split(',')[0], v.replace(\"'\",\"\").split(',')[1]\n",
        "      label  = emotion\n",
        "             \n",
        "      f.write('\\n'+ str(label))\n",
        "      #print('\\n'+name+' '+str(label))\n",
        "      f.close()\n",
        "             \n",
        "  print('\\nTest out created.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIFgPe8rnU0j"
      },
      "source": [
        "'''\n",
        "Create a .csv file with frame path/label(taken from 'inference.txt')\n",
        "crate_test_output will output 223 .txt files with the predictions\n",
        "'''\n",
        "\n",
        "create_test_output('/Test_Set/test_set_inference.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}